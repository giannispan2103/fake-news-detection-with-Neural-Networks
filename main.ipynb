{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import TextRNN, ProjectedAttentionTextTitleAuthorRNN\n",
    "from preprocess import generate_data\n",
    "from utils import train\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_text():\n",
    "    data = generate_data(split_point=15000, emb_size=50, maxlen=100)\n",
    "    emb_matrix = data['emb_matrix']\n",
    "    train_batches = data['train_batches']\n",
    "    test_batches = data['test_batches']\n",
    "    model = TextRNN(emb_matrix, trainable_embeddings=False).cuda()\n",
    "    optimizer = Adam(model.params, 0.001)\n",
    "    criterion = BCEWithLogitsLoss()\n",
    "    train(model, train_batches, test_batches, optimizer, criterion, 50, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_text_title_author():\n",
    "    data = generate_data(split_point=15000, emb_size=50, maxlen=25)\n",
    "    a2i = data['a2i']\n",
    "    emb_matrix = data['emb_matrix']\n",
    "    train_batches = data['train_batches']\n",
    "    test_batches = data['test_batches']\n",
    "    model = ProjectedAttentionTextTitleAuthorRNN(emb_matrix, author_embeddings_input_size=len(a2i), embeddings_dropout=0.5, top_mlp_dropout=0.5, text_stacked_layers=1, text_cell_hidden_size=128,\n",
    "                                                 title_cell_hidden_size=32, top_mlp_outer_activation=None, top_mlp_layers=2).cuda()\n",
    "    optimizer = Adam(model.params, 0.001)\n",
    "    criterion = BCEWithLogitsLoss()\n",
    "    train(model, train_batches, test_batches, optimizer, criterion, 50, 5,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posts for training: 15000\n",
      "posts for testing: 5387\n",
      "tokens found in training data set: 154164\n",
      "tokens with frequency >= 1: 154164\n",
      "embedded tokens with frequency >= 1: 102016\n",
      "no embedding for:  *$*PAD*$*\n",
      "no embedding for:  *$*UNK*$*\n",
      "epoch 1, auc: 81.803. Time: 0 minutes, 3 seconds\n",
      "epoch 2, auc: 88.999. Time: 0 minutes, 1 seconds\n",
      "best epoch so far\n",
      "epoch 3, auc: 92.566. Time: 0 minutes, 1 seconds\n",
      "best epoch so far\n",
      "epoch 4, auc: 94.004. Time: 0 minutes, 1 seconds\n",
      "best epoch so far\n",
      "epoch 5, auc: 94.930. Time: 0 minutes, 1 seconds\n",
      "best epoch so far\n",
      "epoch 6, auc: 95.387. Time: 0 minutes, 1 seconds\n",
      "best epoch so far\n",
      "epoch 7, auc: 95.784. Time: 0 minutes, 1 seconds\n",
      "best epoch so far\n",
      "epoch 8, auc: 96.018. Time: 0 minutes, 1 seconds\n",
      "best epoch so far\n",
      "epoch 9, auc: 96.233. Time: 0 minutes, 1 seconds\n",
      "best epoch so far\n",
      "epoch 10, auc: 96.507. Time: 0 minutes, 1 seconds\n",
      "best epoch so far\n",
      "epoch 11, auc: 96.546. Time: 0 minutes, 1 seconds\n",
      "best epoch so far\n",
      "epoch 12, auc: 96.553. Time: 0 minutes, 1 seconds\n",
      "best epoch so far\n",
      "epoch 13, auc: 96.645. Time: 0 minutes, 1 seconds\n",
      "best epoch so far\n",
      "epoch 14, auc: 96.470. Time: 0 minutes, 1 seconds\n",
      "epoch 15, auc: 96.601. Time: 0 minutes, 1 seconds\n",
      "epoch 16, auc: 96.385. Time: 0 minutes, 1 seconds\n",
      "epoch 17, auc: 96.447. Time: 0 minutes, 1 seconds\n",
      "epoch 18, auc: 96.475. Time: 0 minutes, 1 seconds\n"
     ]
    }
   ],
   "source": [
    "run_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posts for training: 15000\n",
      "posts for testing: 5387\n",
      "tokens found in training data set: 232230\n",
      "tokens with frequency >= 1: 232230\n",
      "embedded tokens with frequency >= 1: 95396\n",
      "no embedding for:  *$*PAD*$*\n",
      "no embedding for:  *$*UNK*$*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramFiles\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Users\\User\\Documents\\Python Scripts\\fake-news-detection-with-Neural-Networks\\modules.py:193: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  att_weights = self.at_softmax(att_sc_dist).unsqueeze(2)\n",
      "E:\\ProgramFiles\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, auc: 96.221. Time: 0 minutes, 4 seconds\n",
      "epoch 2, auc: 98.067. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 3, auc: 98.489. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 4, auc: 98.859. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 5, auc: 98.978. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 6, auc: 99.091. Time: 0 minutes, 3 seconds\n",
      "best epoch so far\n",
      "epoch 7, auc: 99.122. Time: 0 minutes, 3 seconds\n",
      "best epoch so far\n",
      "epoch 8, auc: 99.225. Time: 0 minutes, 3 seconds\n",
      "best epoch so far\n",
      "epoch 9, auc: 99.274. Time: 0 minutes, 3 seconds\n",
      "best epoch so far\n",
      "epoch 10, auc: 99.395. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 11, auc: 99.421. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 12, auc: 99.414. Time: 0 minutes, 2 seconds\n",
      "epoch 13, auc: 99.538. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 14, auc: 99.560. Time: 0 minutes, 3 seconds\n",
      "best epoch so far\n",
      "epoch 15, auc: 99.628. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 16, auc: 99.658. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 17, auc: 99.706. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 18, auc: 99.721. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 19, auc: 99.764. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 20, auc: 99.807. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 21, auc: 99.834. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 22, auc: 99.829. Time: 0 minutes, 2 seconds\n",
      "epoch 23, auc: 99.841. Time: 0 minutes, 3 seconds\n",
      "best epoch so far\n",
      "epoch 24, auc: 99.893. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 25, auc: 99.927. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 26, auc: 99.901. Time: 0 minutes, 2 seconds\n",
      "epoch 27, auc: 99.902. Time: 0 minutes, 2 seconds\n",
      "epoch 28, auc: 99.935. Time: 0 minutes, 3 seconds\n",
      "best epoch so far\n",
      "epoch 29, auc: 99.944. Time: 0 minutes, 3 seconds\n",
      "best epoch so far\n",
      "epoch 30, auc: 99.945. Time: 0 minutes, 3 seconds\n",
      "best epoch so far\n",
      "epoch 31, auc: 99.953. Time: 0 minutes, 3 seconds\n",
      "best epoch so far\n",
      "epoch 32, auc: 99.960. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 33, auc: 99.963. Time: 0 minutes, 2 seconds\n",
      "best epoch so far\n",
      "epoch 34, auc: 99.954. Time: 0 minutes, 4 seconds\n",
      "epoch 35, auc: 99.885. Time: 0 minutes, 4 seconds\n",
      "epoch 36, auc: 99.962. Time: 0 minutes, 4 seconds\n",
      "epoch 37, auc: 99.968. Time: 0 minutes, 4 seconds\n",
      "best epoch so far\n",
      "epoch 38, auc: 99.968. Time: 0 minutes, 5 seconds\n",
      "best epoch so far\n",
      "epoch 39, auc: 99.962. Time: 0 minutes, 4 seconds\n",
      "epoch 40, auc: 99.969. Time: 0 minutes, 4 seconds\n",
      "best epoch so far\n",
      "epoch 41, auc: 99.981. Time: 0 minutes, 4 seconds\n",
      "best epoch so far\n",
      "epoch 42, auc: 99.979. Time: 0 minutes, 4 seconds\n",
      "epoch 43, auc: 99.976. Time: 0 minutes, 4 seconds\n",
      "epoch 44, auc: 99.974. Time: 0 minutes, 4 seconds\n",
      "epoch 45, auc: 99.979. Time: 0 minutes, 4 seconds\n",
      "epoch 46, auc: 99.967. Time: 0 minutes, 4 seconds\n"
     ]
    }
   ],
   "source": [
    "run_text_title_author()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
